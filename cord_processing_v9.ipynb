{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cord-processing-v19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTXnZcesoqF7Px5bkZmVtJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qDcMoKFzM8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install googletrans\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U scispacy\n",
        "!pip install spacy_langdetect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBdZEX4UPj6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from googletrans import Translator\n",
        "import pandas as pd \n",
        "import os\n",
        "import numpy as np\n",
        "import scispacy\n",
        "import json\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.spatial import distance\n",
        "import ipywidgets as widgets\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from spacy_langdetect import LanguageDetector\n",
        "# UMLS linking will find concepts in the text, and link them to UMLS. \n",
        "from scispacy.umls_linking import UmlsEntityLinker\n",
        "import time\n",
        "from spacy.vocab import Vocab\n",
        "from multiprocessing import Process, Queue, Manager\n",
        "from multiprocessing.pool import Pool\n",
        "from functools import partial\n",
        "import re\n",
        "import ast"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdqlPlguQq08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(text):\n",
        "    translator=Translator(dest='en')\n",
        "    translation=translator.translate(str(text)).text\n",
        "    return translation\n",
        "\n",
        "# Returns a dictionary object that's easy to parse in pandas. For tables! :D\n",
        "def extract_tables_from_json(js):\n",
        "    json_list = []\n",
        "    # Figures contain useful information. Since NLP doesn't handle images and tables,\n",
        "    # we can leverage this text data in lieu of visual data.\n",
        "    for figure in list(js[\"ref_entries\"].keys()):\n",
        "        json_dict = [\"figref\", figure, js[\"ref_entries\"][figure][\"text\"]]\n",
        "        json_dict.append(json_dict)\n",
        "    return json_list\n",
        "\n",
        "def init_filter_dict(): \n",
        "    inverse = dict() \n",
        "    d = {\n",
        "        \"discussion\": [\"conclusions\",\"conclusion\",'| discussion', \"discussion\",  'concluding remarks',\n",
        "                       'discussion and conclusions','conclusion:', 'discussion and conclusion',\n",
        "                       'conclusions:', 'outcomes', 'conclusions and perspectives', \n",
        "                       'conclusions and future perspectives', 'conclusions and future directions'],\n",
        "        \"results\": ['executive summary', 'result', 'summary','results','results and discussion','results:',\n",
        "                    'comment',\"findings\"],\n",
        "        \"introduction\": ['introduction', 'background', 'i. introduction','supporting information','| introduction'],\n",
        "        \"methods\": ['methods','method','statistical methods','materials','materials and methods',\n",
        "                    'data collection','the study','study design','experimental design','objective',\n",
        "                    'objectives','procedures','data collection and analysis', 'methodology',\n",
        "                    'material and methods','the model','experimental procedures','main text',],\n",
        "        \"statistics\": ['data analysis','statistical analysis', 'analysis','statistical analyses', \n",
        "                       'statistics','data','measures'],\n",
        "        \"clinical\": ['diagnosis', 'diagnostic features', \"differential diagnoses\", 'classical signs','prognosis', 'clinical signs', 'pathogenesis',\n",
        "                     'etiology','differential diagnosis','clinical features', 'case report', 'clinical findings',\n",
        "                     'clinical presentation'],\n",
        "        'treatment': ['treatment', 'interventions'],\n",
        "        \"prevention\": ['epidemiology','risk factors'],\n",
        "        \"subjects\": ['demographics','samples','subjects', 'study population','control','patients', \n",
        "                   'participants','patient characteristics'],\n",
        "        \"animals\": ['animals','animal models'],\n",
        "        \"abstract\": [\"abstract\", 'a b s t r a c t','author summary'], \n",
        "        \"review\": ['review','literature review','keywords']}\n",
        "    \n",
        "    for key in d: \n",
        "        # Go through the list that is saved in the dict:\n",
        "        for item in d[key]:\n",
        "            # Check if in the inverted dict the key exists\n",
        "            if item not in inverse: \n",
        "                # If not create a new list\n",
        "                inverse[item] = [key] \n",
        "            else: \n",
        "                inverse[item].append(key) \n",
        "    return inverse\n",
        "\n",
        "inverted_dict = init_filter_dict()\n",
        "    \n",
        "def get_section_name(text):\n",
        "    if len(text) == 0:\n",
        "        return(text)\n",
        "    text = text.lower()\n",
        "    if text in inverted_dict.keys():\n",
        "        return(inverted_dict[text][0])\n",
        "    else:\n",
        "        if \"case\" in text or \"study\" in text: \n",
        "            return(\"methods\")\n",
        "        elif \"clinic\" in text:\n",
        "            return(\"clinical\")\n",
        "        elif \"stat\" in text:\n",
        "            return(\"statistics\")\n",
        "        elif \"intro\" in text or \"backg\" in text:\n",
        "            return(\"introduction\")\n",
        "        elif \"data\" in text:\n",
        "            return(\"statistics\")\n",
        "        elif \"discuss\" in text:\n",
        "            return(\"discussion\")\n",
        "        elif \"patient\" in text:\n",
        "            return(\"subjects\")\n",
        "        else: \n",
        "            return(text)\n",
        "\n",
        "def init_nlp():\n",
        "    nlp = spacy.load(\"/home/acorn/Downloads/en_core_sci_lg-0.2.4/en_core_sci_lg/en_core_sci_lg-0.2.4/\", disable=[\"tagger\"])\n",
        "    nlp.max_length=2000000\n",
        "\n",
        "    # We also need to detect language, or else we'll be parsing non-english text \n",
        "    # as if it were English. \n",
        "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "\n",
        "    # Add the abbreviation pipe to the spacy pipeline. Only need to run this once.\n",
        "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
        "    nlp.add_pipe(abbreviation_pipe)\n",
        "\n",
        "    # Our linker will look up named entities/concepts in the UMLS graph and normalize\n",
        "    # the data for us. \n",
        "    linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
        "    nlp.add_pipe(linker)\n",
        "    \n",
        "    new_vector = nlp(\n",
        "               \"\"\"Positive-sense singleâ€stranded ribonucleic acid virus, subgenus \n",
        "                   sarbecovirus of the genus Betacoronavirus. \n",
        "                   Also known as severe acute respiratory syndrome coronavirus 2, \n",
        "                   also known by 2019 novel coronavirus. It is \n",
        "                   contagious in humans and is the cause of the ongoing pandemic of \n",
        "                   coronavirus disease. Coronavirus disease 2019 is a zoonotic infectious \n",
        "                   disease.\"\"\").vector\n",
        "\n",
        "    vector_data = {\"COVID-19\": new_vector,\n",
        "               \"2019-nCoV\": new_vector,\n",
        "               \"SARS-CoV-2\": new_vector}\n",
        "\n",
        "    vocab = Vocab()\n",
        "    for word, vector in vector_data.items():\n",
        "        nlp.vocab.set_vector(word, vector)\n",
        "    \n",
        "    return(nlp, linker)\n",
        "def init_ner():\n",
        "    models = [\"en_ner_craft_md\", \"en_ner_jnlpba_md\",\"en_ner_bc5cdr_md\",\"en_ner_bionlp13cg_md\"]\n",
        "    nlps = [spacy.load(model) for model in models]\n",
        "    return(nlps)\n",
        "\n",
        "def process_metadata(directory):\n",
        "    rows = []\n",
        "    if directory[-1] != \"/\": \n",
        "        directory = directory + \"/\"\n",
        "        \n",
        "    df1 = pd.read_csv(directory + \"metadata_old.csv\")\n",
        "    df2 = pd.read_csv(directory + \"metadata.csv\")\n",
        "    df = df2[~df2[\"cord_uid\"].isin(df1[\"cord_uid\"])] \n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    del df1\n",
        "    del df2\n",
        "    \n",
        "    df.fillna(\"~\", inplace=True)\n",
        "    for i in df[df[\"has_pmc_xml_parse\"] == 1].index:\n",
        "        section = (str(df.iloc[i].full_text_file) + \"/\") * 2\n",
        "        pmcid = df.iloc[i].pmcid\n",
        "        filename = directory + section + \"pmc_json/\" + pmcid + \".xml.json\"\n",
        "        try: \n",
        "            with open(filename) as paperjs:\n",
        "                jsfile = json.load(paperjs)\n",
        "        except:\n",
        "            print(\"Problem with\", df.iloc[i].cord_uid)\n",
        "            continue\n",
        "\n",
        "        _id = df.iloc[i][\"cord_uid\"]\n",
        "        if \"title\" in jsfile.keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
        "        else:\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
        "        if \"abstract\" in jsfile.keys():\n",
        "            if len(jsfile[\"abstract\"]) > 1:\n",
        "                for j in range(len(jsfile[\"abstract\"])):\n",
        "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
        "            else:\n",
        "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
        "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
        "        else: \n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
        "\n",
        "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
        "\n",
        "        for section in sections: \n",
        "            for l in range(len(jsfile[\"body_text\"])):\n",
        "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
        "                    if section == '':\n",
        "                        section = \"body_text\"\n",
        "                    rows.append(dict(cord_uid=_id, section=section, \n",
        "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
        "\n",
        "        tables = extract_tables_from_json(jsfile)\n",
        "        for table in tables:\n",
        "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
        "\n",
        "\n",
        "    for i in df[(df[\"has_pmc_xml_parse\"] == 0) & (df[\"has_pdf_parse\"] == 1)].index:\n",
        "        section = (str(df.iloc[i].full_text_file) + \"/\") * 2\n",
        "        sha = df.iloc[i].sha\n",
        "        if len(sha.split(\"; \")) > 1:\n",
        "            sha = sha.split(\"; \")[0]\n",
        "        filename = directory + section + \"pdf_json/\" + sha + \".json\"\n",
        "        try:\n",
        "            with open(filename) as paperjs:\n",
        "                jsfile = json.load(paperjs)\n",
        "        except:\n",
        "            print(\"Problem with\", df.iloc[i].cord_uid)\n",
        "\n",
        "        _id = df.iloc[i][\"cord_uid\"]\n",
        "        if \"title\" in jsfile.keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
        "        else:\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
        "        if \"abstract\" in jsfile.keys():\n",
        "            if len(jsfile[\"abstract\"]) > 1:\n",
        "                for j in range(len(jsfile[\"abstract\"])):\n",
        "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
        "            else:\n",
        "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
        "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
        "        else: \n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
        "\n",
        "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
        "\n",
        "        for section in sections: \n",
        "            for l in range(len(jsfile[\"body_text\"])):\n",
        "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
        "                    if section == '':\n",
        "                        section = \"body_text\"\n",
        "                    rows.append(dict(cord_uid=_id, section=section, \n",
        "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
        "\n",
        "        tables = extract_tables_from_json(jsfile)\n",
        "        for table in tables:\n",
        "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
        "\n",
        "    for i in df[(df[\"has_pmc_xml_parse\"] == 0) & (df[\"has_pdf_parse\"] == 0)].index:\n",
        "        section = (str(df.iloc[i].full_text_file) + \"/\") * 2\n",
        "        sha = df.iloc[i].sha\n",
        "\n",
        "        if len(sha.split(\"; \")) > 1:\n",
        "            sha = sha.split(\"; \")[0]\n",
        "        filename = directory + section + \"pdf_json/\" + sha + \".json\"\n",
        "\n",
        "        if len(sha) < 2: \n",
        "            bad_sha = True\n",
        "            try:\n",
        "                with open(directory + section + \"pmc_json/\" + df.iloc[i][\"pmcid\"] + \".xml.json\") as paperjs:\n",
        "                    jsfile = json.load(paperjs)\n",
        "            except:\n",
        "                pass\n",
        "        if bad_sha == True:\n",
        "            bad_sha = False\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(filename) as paperjs:\n",
        "                jsfile = json.load(paperjs)\n",
        "        except:\n",
        "            print(\"Problem with \", df.iloc[i].cord_uid)\n",
        "            continue\n",
        "\n",
        "        _id = df.iloc[i][\"cord_uid\"]\n",
        "        if \"title\" in jsfile.keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
        "        else:\n",
        "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
        "        if \"abstract\" in jsfile.keys():\n",
        "            if len(jsfile[\"abstract\"]) > 1:\n",
        "                for j in range(len(jsfile[\"abstract\"])):\n",
        "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
        "            else:\n",
        "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
        "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
        "        else: \n",
        "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
        "\n",
        "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
        "\n",
        "        for section in sections: \n",
        "            for l in range(len(jsfile[\"body_text\"])):\n",
        "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
        "                    if section == '':\n",
        "                        section = \"body_text\"\n",
        "                    rows.append(dict(cord_uid=_id, section=section, \n",
        "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
        "\n",
        "        tables = extract_tables_from_json(jsfile)\n",
        "        for table in tables:\n",
        "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
        "            \n",
        "    processed_ids = [d[\"cord_uid\"] for d in rows]\n",
        "    \n",
        "    for i in df[~df[\"cord_uid\"].isin(processed_ids)].index:\n",
        "        rows.append(dict(cord_uid=df.iloc[i][\"cord_uid\"], section=\"title\", subsection=0, text=df.iloc[i][\"title\"]))\n",
        "        rows.append(dict(cord_uid=df.iloc[i][\"cord_uid\"], section=\"abstract\", subsection=0, text=df.iloc[i][\"abstract\"]))\n",
        "\n",
        "    return(pd.DataFrame(rows))\n",
        "\n",
        "def parallelize_dataframe(df, func, n_cores=6, n_parts=400):\n",
        "    df_split = np.array_split(df, n_parts)\n",
        "    pool = Pool(n_cores)\n",
        "    list(tqdm(pool.imap_unordered(func, df_split), total=len(df_split)))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "                    \n",
        "def init_list_cols():\n",
        "    return ['GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', 'PROTEIN', \n",
        "                          'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID',\n",
        "                          'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', 'IMMATERIAL_ANATOMICAL_ENTITY',\n",
        "                          'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION',\n",
        "                          'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION', \"lemma\", \"UMLS\",\"UMLS_ID\"]\n",
        "        \n",
        "def pipeline(df):\n",
        "    \n",
        "    name = df.iloc[0][\"cord_uid\"] + str(df.iloc[0][\"subsection\"])+ \"0\" + \".pickle\"\n",
        "                    \n",
        "    if not os.path.exists(\"df_parts/\"):\n",
        "        os.mkdir(\"df_parts/\")\n",
        "        \n",
        "    if name in os.listdir(\"df_parts/\"):\n",
        "        return True\n",
        "    languages = []\n",
        "    start_chars = []\n",
        "    end_chars = []\n",
        "    entities = []\n",
        "    sentences = []\n",
        "    lemmas = []\n",
        "    vectors = []\n",
        "    subsections = []\n",
        "    _ids = []\n",
        "    columns = []\n",
        "    nlp, linker = init_nlp()\n",
        "    nlps = init_ner()\n",
        "    translated = []\n",
        "    umls_ids = []\n",
        "\n",
        "    scispacy_ent_types = ['GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', 'PROTEIN', \n",
        "                          'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID',\n",
        "                          'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', 'IMMATERIAL_ANATOMICAL_ENTITY',\n",
        "                          'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION',\n",
        "                          'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION']\n",
        "    \n",
        "    for i in tqdm(range(len(df))):\n",
        "        doc = nlp(str(df.iloc[i][\"text\"]))\n",
        "        sents = [sent for sent in doc.sents]\n",
        "\n",
        "        if len(doc._.abbreviations) > 0 and doc._.language[\"language\"] == \"en\":\n",
        "            doc._.abbreviations.sort()\n",
        "            join_list = []\n",
        "            start = 0\n",
        "            for abbrev in doc._.abbreviations:\n",
        "                join_list.append(str(doc.text[start:abbrev.start_char]))\n",
        "                if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n",
        "                    join_list.append(str(abbrev._.long_form))\n",
        "                else:\n",
        "                    join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n",
        "                start = abbrev.end_char\n",
        "            # Reassign fixed body text to article in df.\n",
        "            new_text = \"\".join(join_list)\n",
        "            # We have new text. Re-nlp the doc for futher processing!\n",
        "            doc = nlp(new_text)\n",
        "\n",
        "        if doc._.language[\"language\"] == \"en\" and len(doc.text) > 5:\n",
        "            sents = [sent for sent in doc.sents if len(sent) > 5]\n",
        "            for sent in sents:\n",
        "                languages.append(doc._.language[\"language\"])\n",
        "                sentences.append(sent.text)\n",
        "                vectors.append(sent.vector)\n",
        "                translated.append(False)\n",
        "                subsections.append(df.iloc[i][\"subsection\"])\n",
        "                lemmas.append([token.lemma_.lower() for token in sent if not token.is_stop and re.search('[a-zA-Z]', str(token))])\n",
        "                doc_ents = []\n",
        "                for ent in sent.ents: \n",
        "                    if len(ent._.umls_ents) > 0:\n",
        "                        poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n",
        "                        doc_ents.append(poss)\n",
        "                entities.append(doc_ents)\n",
        "                umls_ids.append([entity._.umls_ents[0][0] for entity in sent.ents if len(entity._.umls_ents) > 0])\n",
        "                _ids.append(df.iloc[i][\"cord_uid\"])\n",
        "                columns.append(df.iloc[i][\"section\"])\n",
        "        else:  \n",
        "            try: \n",
        "                text = translate(df.iloc[i][\"text\"])\n",
        "                doc = nlp(str(df.iloc[i][\"text\"]))\n",
        "                sents = [sent for sent in doc.sents]\n",
        "\n",
        "                if len(doc._.abbreviations) > 0:\n",
        "                    doc._.abbreviations.sort()\n",
        "                    join_list = []\n",
        "                    start = 0\n",
        "                    for abbrev in doc._.abbreviations:\n",
        "                        join_list.append(str(doc.text[start:abbrev.start_char]))\n",
        "                        if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n",
        "                            join_list.append(str(abbrev._.long_form))\n",
        "                        else:\n",
        "                            join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n",
        "                        start = abbrev.end_char\n",
        "                    # Reassign fixed body text to article in df.\n",
        "                    new_text = \"\".join(join_list)\n",
        "                    # We have new text. Re-nlp the doc for futher processing!\n",
        "                    doc = nlp(new_text)\n",
        "\n",
        "                if len(doc.text) > 5:\n",
        "                    sents = [sent for sent in doc.sents if len(sent) > 5]\n",
        "                    for sent in sents:\n",
        "                        languages.append(doc._.language[\"language\"])\n",
        "                        sentences.append(sent.text)\n",
        "                        vectors.append(sent.vector)\n",
        "                        translated.append(True)\n",
        "                        subsections.append(df.iloc[i][\"subsection\"])\n",
        "                        lemmas.append([token.lemma_ for token in doc if not token.is_stop and re.search('[a-zA-Z]', str(token))])\n",
        "                        doc_ents = []\n",
        "                        for ent in sent.ents: \n",
        "                            if len(ent._.umls_ents) > 0:\n",
        "                                poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n",
        "                                doc_ents.append(poss)\n",
        "                        umls_ids.append([entity._.umls_ents[0][0] for entity in sent.ents if len(entity._.umls_ents) > 0])\n",
        "                        entities.append(doc_ents)\n",
        "                        _ids.append(df.iloc[i][\"cord_uid\"])\n",
        "                        columns.append(df.iloc[i][\"section\"])\n",
        "                        \n",
        "            except:\n",
        "                entities.append(\"[]\")\n",
        "                translated.append(False)\n",
        "                subsections.append(df.iloc[i][\"subsection\"])\n",
        "                sentences.append(doc.text)\n",
        "                vectors.append(np.zeros(200))\n",
        "                lemmas.append(\"[]\")\n",
        "                _ids.append(df.iloc[i,0])\n",
        "                umls_ids.append(\"[]\")\n",
        "                languages.append(doc._.language[\"language\"])\n",
        "                columns.append(df.iloc[i][\"section\"])\n",
        "    \n",
        "    li1 = _ids\n",
        "    li2 = subsections\n",
        "    li3 = [i for i in range(len(entities))]\n",
        "    \n",
        "    sentence_id = [str(x) + str(y) + str(z)  for x,y,z in zip(li1,li2,li3)]\n",
        "\n",
        "    new_df = pd.DataFrame(data={\"cord_uid\": _ids, \"language\": languages, \"sentence_id\": sentence_id,\n",
        "                                \"section\": columns, \"subsection\":subsections, \"sentence\": sentences,\n",
        "                                \"lemma\": lemmas, \"UMLS\": entities, \"UMLS_IDS\": umls_ids,\n",
        "                                \"w2vVector\": vectors, \"translated\":translated})\n",
        "            \n",
        "\n",
        "    \n",
        "    for col in scispacy_ent_types:\n",
        "        new_df[col] = \"[]\"\n",
        "    for j in tqdm(new_df.index):\n",
        "        for nlp in nlps:\n",
        "            doc = nlp(str(new_df.iloc[j][\"sentence\"]))\n",
        "            keys = list(set([ent.label_ for ent in doc.ents]))\n",
        "            for key in keys:\n",
        "\n",
        "                # Some entity types are present in the model, but not in the documentation! \n",
        "                # In that case, we'll just automatically add it to the df. \n",
        "                if key not in scispacy_ent_types:\n",
        "                    new_df = pd.concat([new_df,pd.DataFrame(columns=[key])])\n",
        "                    new_df[key] = \"[]\"\n",
        "\n",
        "                values = [ent.text for ent in doc.ents if ent.label_ == key]\n",
        "                new_df.at[j,key] = values\n",
        "\n",
        "                \n",
        "    new_df[\"w2vVector\"] = [np.asarray(a=i, dtype=\"float64\") for i in new_df[\"w2vVector\"].to_list()]\n",
        "\n",
        "    \n",
        "    new_df.to_pickle(\"df_parts/\" + new_df.iloc[0][\"sentence_id\"] + \".pickle\", compression=\"gzip\")\n",
        "    #new_df.drop(columns=[\"w2vVector\"]).to_pickle(\"df_parts/\" + new_df.iloc[0][\"sentence_id\"] + \".ptext\", compression=\"gzip\")\n",
        "    #new_df[[\"sentence_id\",\"w2vVector\"]].to_pickle(\"df_parts/\" + new_df.iloc[0][\"sentence_id\"] + \".pvec\", compression=\"gzip\")\n",
        "In [19]:\n",
        "# Change this to where you have the metadata file. Make sure to untar/unzip all the folders.\n",
        "directory = \"/home/acorn/Documents/covid/CORD-19-research-challenge/\"\n",
        "\n",
        "# This method will parse the metadata, add all JSON information according to the highest\n",
        "# quality source available (Metadata > XML Parse > PDF Parse > leftovers in Metadata)\n",
        "# The function returns a pandas dataframe. \n",
        "\n",
        "df = process_metadata(directory)\n",
        "\n",
        "# Remove the rows where text was unavailable\n",
        "df = df[df[\"text\"] != \"~\"]\n",
        "\n",
        "# Save the processed data however you like\n",
        "df.to_pickle(\"v9_dataset.pkl\", compression=\"gzip\")bv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
